{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2023-04-20 02:38:14,551 - modelscope - INFO - PyTorch version 1.12.1+cu116 Found.\n",
      "2023-04-20 02:38:14,562 - modelscope - INFO - TensorFlow version 2.10.1 Found.\n",
      "2023-04-20 02:38:14,563 - modelscope - INFO - Loading ast index from C:\\Users\\Administrator\\.cache\\modelscope\\ast_indexer\n",
      "2023-04-20 02:38:15,046 - modelscope - INFO - Loading done! Current index file version is 1.3.0, with md5 3e9bc5fbe035d6035638439c5feaf9d4 and a total number of 746 components indexed\n",
      "2023-04-20 02:38:19,035 - modelscope - INFO - Use user-specified model revision: v1.0.1\n",
      "2023-04-20 02:38:19,583 - modelscope - INFO - File chinese_vocab.txt already in cache, skip downloading!\n",
      "2023-04-20 02:38:19,583 - modelscope - INFO - File configuration.json already in cache, skip downloading!\n",
      "2023-04-20 02:38:19,585 - modelscope - INFO - File dict.src.txt already in cache, skip downloading!\n",
      "2023-04-20 02:38:19,585 - modelscope - INFO - File dict.tgt.txt already in cache, skip downloading!\n",
      "2023-04-20 02:38:19,587 - modelscope - INFO - File model.jpg already in cache, skip downloading!\n",
      "2023-04-20 02:38:19,591 - modelscope - INFO - File pytorch_model.pt already in cache, skip downloading!\n",
      "2023-04-20 02:38:19,592 - modelscope - INFO - File README.md already in cache, skip downloading!\n",
      "2023-04-20 02:38:19,614 - modelscope - INFO - initiate model from C:\\Users\\Administrator\\.cache\\modelscope\\hub\\damo\\nlp_bart_text-error-correction_chinese\n",
      "2023-04-20 02:38:19,615 - modelscope - INFO - initiate model from location C:\\Users\\Administrator\\.cache\\modelscope\\hub\\damo\\nlp_bart_text-error-correction_chinese.\n",
      "2023-04-20 02:38:19,620 - modelscope - INFO - initialize model from C:\\Users\\Administrator\\.cache\\modelscope\\hub\\damo\\nlp_bart_text-error-correction_chinese\n",
      "2023-04-20 02:38:21 | INFO | fairseq.tasks.text_to_speech | Please install tensorboardX: pip install tensorboardX\n",
      "2023-04-20 02:38:54 | INFO | fairseq.tasks.translation | [src] dictionary: 21132 types\n",
      "2023-04-20 02:38:54 | INFO | fairseq.tasks.translation | [tgt] dictionary: 21132 types\n",
      "2023-04-20 02:39:02,990 - modelscope - WARNING - No preprocessor field found in cfg.\n",
      "2023-04-20 02:39:02 | WARNING | modelscope | No preprocessor field found in cfg.\n",
      "2023-04-20 02:39:02,992 - modelscope - WARNING - No val key and type key found in preprocessor domain of configuration.json file.\n",
      "2023-04-20 02:39:02 | WARNING | modelscope | No val key and type key found in preprocessor domain of configuration.json file.\n",
      "2023-04-20 02:39:02,993 - modelscope - WARNING - Cannot find available config to build preprocessor at mode inference, current config: {'model_dir': 'C:\\\\Users\\\\Administrator\\\\.cache\\\\modelscope\\\\hub\\\\damo\\\\nlp_bart_text-error-correction_chinese'}. trying to build by task and model information.\n",
      "2023-04-20 02:39:02 | WARNING | modelscope | Cannot find available config to build preprocessor at mode inference, current config: {'model_dir': 'C:\\\\Users\\\\Administrator\\\\.cache\\\\modelscope\\\\hub\\\\damo\\\\nlp_bart_text-error-correction_chinese'}. trying to build by task and model information.\n",
      "2023-04-20 02:39:03,037 - modelscope - WARNING - No preprocessor field found in cfg.\n",
      "2023-04-20 02:39:03 | WARNING | modelscope | No preprocessor field found in cfg.\n",
      "2023-04-20 02:39:03,039 - modelscope - WARNING - No val key and type key found in preprocessor domain of configuration.json file.\n",
      "2023-04-20 02:39:03 | WARNING | modelscope | No val key and type key found in preprocessor domain of configuration.json file.\n",
      "2023-04-20 02:39:03,039 - modelscope - WARNING - Cannot find available config to build preprocessor at mode inference, current config: {'model_dir': 'C:\\\\Users\\\\Administrator\\\\.cache\\\\modelscope\\\\hub\\\\damo\\\\nlp_bart_text-error-correction_chinese'}. trying to build by task and model information.\n",
      "2023-04-20 02:39:03 | WARNING | modelscope | Cannot find available config to build preprocessor at mode inference, current config: {'model_dir': 'C:\\\\Users\\\\Administrator\\\\.cache\\\\modelscope\\\\hub\\\\damo\\\\nlp_bart_text-error-correction_chinese'}. trying to build by task and model information.\n"
     ]
    }
   ],
   "source": [
    "from modelscope.pipelines import pipeline\n",
    "from modelscope.utils.constant import Tasks\n",
    "\n",
    "p = pipeline(Tasks.text_error_correction,  'damo/nlp_bart_text-error-correction_chinese',model_revision='v1.0.1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\_distutils_hack\\__init__.py:33: UserWarning: Setuptools is replacing distutils.\n",
      "  warnings.warn(\"Setuptools is replacing distutils.\")\n",
      "\u001b[33m[2023-04-20 02:39:06,806] [ WARNING]\u001b[0m - Detected that datasets module was imported before paddlenlp. This may cause PaddleNLP datasets to be unavalible in intranet. Please import paddlenlp before datasets module to avoid download issues\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from paddlenlp import Taskflow\n",
    "tagger = Taskflow(\"pos_tagging\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "import difflib\n",
    "def chinese_word_segmentation(text,cut_all_mode=1):\n",
    "    seg_list = jieba.cut(text,cut_all=cut_all_mode)\n",
    "    return \" \".join(seg_list)\n",
    "\n",
    "def smallest_word(s_ls, t_ls): \n",
    "    for i in range(len(t_ls)):\n",
    "        for j in range(len(t_ls)):\n",
    "            if i == len(t_ls) or j == len(t_ls):\n",
    "                break\n",
    "            if i != j:\n",
    "                if t_ls[i] in t_ls[j]:\n",
    "                    t_ls.pop(j)\n",
    "\n",
    "    for i in range(len(s_ls)):\n",
    "        for j in range(len(s_ls)):\n",
    "            if i == len(s_ls) or j == len(s_ls):\n",
    "                break\n",
    "            if i != j:\n",
    "                if s_ls[i] in s_ls[j]:\n",
    "                    s_ls.pop(j)\n",
    "    return s_ls,t_ls\n",
    "\n",
    "def get_changes(source, target):\n",
    "    a = chinese_word_segmentation(source)\n",
    "    b = chinese_word_segmentation(target)\n",
    "\n",
    "    s_ls = [i for i in a.split() if i not in b.split()]\n",
    "    t_ls =[i for i in b.split() if i not in a.split()]\n",
    "    return smallest_word(s_ls,t_ls)\n",
    "\n",
    "def format_target_list(texts):\n",
    "    target_list = [{'output': text} for text in texts]\n",
    "    return target_list\n",
    "\n",
    "def get_changed_word_in_sentence(sentence, char_pos):\n",
    "    seg = chinese_word_segmentation(sentence,0)\n",
    "    count = 0\n",
    "    for i in seg.split():\n",
    "        count += len(i)\n",
    "        if count > char_pos:\n",
    "            return i\n",
    "#and type \n",
    "def get_changes_difflib(a,b,mode='char'):\n",
    "    diff = difflib.SequenceMatcher(None, a, b)\n",
    "    opcodes = diff.get_opcodes()\n",
    "    changes = []\n",
    "    if mode == 'char':\n",
    "        for tag, i1, i2, j1, j2 in opcodes:\n",
    "            if tag == 'equal':\n",
    "                continue\n",
    "            elif tag == 'delete':\n",
    "                changes.append(('-',tagger(f'{a[i1:i2]}')))\n",
    "            elif tag == 'insert':\n",
    "                changes.append(('+',tagger(f'{b[j1:j2]}')))\n",
    "            else:\n",
    "                changes.append((tagger(f'{a[i1:i2]}'),'->',tagger(f'{b[j1:j2]}')))\n",
    "    else:\n",
    "        for tag, i1, i2, j1, j2 in opcodes:\n",
    "            if tag == 'equal':\n",
    "                continue\n",
    "            elif tag == 'delete':\n",
    "                changes.append(('-',tagger(f'{get_changed_word_in_sentence(a,i1)}')))\n",
    "            elif tag == 'insert':\n",
    "                changes.append(('+',tagger(f'{get_changed_word_in_sentence(b,j1)}')))\n",
    "            else:\n",
    "                changes.append((tagger(f'{get_changed_word_in_sentence(a,i1)}'),'->',tagger(f'{get_changed_word_in_sentence(b,j1)}')))\n",
    "    return changes\n",
    "\n",
    "def res_formatted(origin,results,mode='char'):  # input: origin text and target text\n",
    "    new_res = []\n",
    "\n",
    "    for i in range(len(origin)):\n",
    "\n",
    "        new_res.append({'source': origin[i], 'target': results[i]['output']})\n",
    "\n",
    "    for i in new_res:\n",
    "        if i['source'] == i['target']:\n",
    "            i['changes'] = ''\n",
    "        else:\n",
    "            i['changes'] = get_changes_difflib(i['source'], i['target'],mode)\n",
    "\n",
    "    return new_res\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[([('驰', 'v')], '->', [('翔', 'PER')])]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_changes_difflib('两部直升机在空中飞驰。','两部直升机在空中飞翔。')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "2023-04-20 02:39:11 | DEBUG | jieba | Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\Administrator\\AppData\\Local\\Temp\\jieba.cache\n",
      "2023-04-20 02:39:11 | DEBUG | jieba | Loading model from cache C:\\Users\\Administrator\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.786 seconds.\n",
      "2023-04-20 02:39:12 | DEBUG | jieba | Loading model cost 0.786 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "2023-04-20 02:39:12 | DEBUG | jieba | Prefix dict has been built successfully.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[([('飞驰', 'v')], '->', [('飞翔', 'v')])]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_changes_difflib('两部直升机在空中飞驰。','两部直升机在空中飞翔。',mode='word')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "身心\n"
     ]
    }
   ],
   "source": [
    "a = '年青人的身心发展，特别值得家长的重视。'\n",
    "a_seg = chinese_word_segmentation(a,0)\n",
    "char_pos = 5\n",
    "count = 0\n",
    "for i in a_seg.split():\n",
    "    count += len(i)\n",
    "    if count >= char_pos:\n",
    "        print(i)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'身心'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_changed_word_in_sentence(a,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_texts =['在环游世界的旅程中，我们见闻了许多新奇的事物。',\n",
    " '球员满场飞跑，技巧地控制着皮球。',\n",
    " '交通比较发展的地方，会成为货物的集散地。',\n",
    " '年青人的身心发展，特别值得家长的重视。',\n",
    " '辛亥革命，觉醒了中国人的心声。',\n",
    " '在手术室里，护士正忙于协助医生、消毒仪器，以方便进行手术。',\n",
    " '经过数十小时的学习，我已经熟练了基本的驾驶技术。',\n",
    " '同一件货品，在中港两地，价钱悬殊了一大半。',\n",
    " '连场大雨，在街上走了很短的路，裤管上沾满了泥泞。',\n",
    " '两部直升机在空中飞驰。',\n",
    " '随着经济的发展，人们的生活水平大为改善。',\n",
    " '春风一阵阵吹来，树枝摇曳着，月光、树影一起晃动起来，发出沙沙的响声。',\n",
    " '警方表示有信心扫除软性毒品的祸害和影响。',\n",
    " '学写诗歌，要注意提炼诗的语言和题材。',\n",
    " '千言万语都表达不了我们对老师的感激的情绪。',\n",
    " '经过这次讲课，对大家的启发很大。',\n",
    " '在通讯事业迅速发展的情况下，可以得到的资讯越来越多。',\n",
    " '世界红十字会是致力拯救第三世界国家贫困儿童。',\n",
    " '为了达到目的，不惜任何手段。',\n",
    " '地铁载客量高，而且比其他交通工具更准时抵达目的地的优点。',\n",
    " '在我遇到困难，他总是想办法帮助我解决。',\n",
    " '学校开办各种暑期活动，有朗诵、绘画、书法、烹饪等兴趣小组，举行了学艺比赛。',\n",
    " '我们走过笔直而又漫长的直路，来到目的地。',\n",
    " '中国语言和任何西方语言不同。',\n",
    " '他忽然大声地高叫起来。',\n",
    " '漫山遍野到处都是逃难的人。']\n",
    "\n",
    "res = p(input_texts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "inputs = '住了在哈尔滨,黑龙江哈尔滨,太冷!他做梦的时候他是一个太空人了。而走在回家，开始下雨！决定了是因为我住在美国的中部分（所以说汉语的不多啦）。她没教我这个星期一样。考虑这样的信息我不知道任何年轻人可以买房子。听说那是中国的《水浒传》影响很大。'\n",
    "\n",
    "def split_sentences(text):\n",
    "    return re.split(r'(?<=[？?！!。；;])', text)\n",
    "\n",
    "def get_output(inputs):\n",
    "    tag_meaning = {\n",
    "    'n': 'common noun',\n",
    "    'f': 'locative noun',\n",
    "    's': 'place noun',\n",
    "    't': 'time noun',\n",
    "    'nr': 'person name',\n",
    "    'ns': 'place name',\n",
    "    'nt': 'organization name',\n",
    "    'nw': 'work name',\n",
    "    'nz': 'other proper noun',\n",
    "    'v': 'verb',\n",
    "    'vd': 'verb adverbial',\n",
    "    'vn': 'verb noun',\n",
    "    'a': 'adjective',\n",
    "    'ad': 'adverbial adjective',\n",
    "    'an': 'adjectival noun',\n",
    "    'd': 'adverb',\n",
    "    'm': 'numeral',\n",
    "    'q': 'quantifier',\n",
    "    'r': 'pronoun',\n",
    "    'p': 'preposition',\n",
    "    'c': 'conjunction',\n",
    "    'u': 'auxiliary',\n",
    "    'xc': 'function words',\n",
    "    'w': 'punctuation',\n",
    "    'PER': 'person name',\n",
    "    'LOC': 'place name',\n",
    "    'ORG': 'organization name',\n",
    "    'TIME': 'time'\n",
    "}\n",
    "    sentences = split_sentences(inputs)\n",
    "    for i in sentences:\n",
    "        if i =='':\n",
    "            sentences.remove(i)\n",
    "    formatted =res_formatted(sentences, p(sentences),mode = 'word')\n",
    "    outputstring = ''\n",
    "    for i in formatted:\n",
    "        outputstring += 'do you mean: ' + i['target']+'\\n'\n",
    "        for j in i['changes']:\n",
    "            if '-' in j:\n",
    "                outputstring +='deleted ' +j[-1][0][0] + '(%s)'% tag_meaning[j[-1][0][1]]+'\\n'\n",
    "            if '+' in j:\n",
    "                outputstring +='adding ' +j[-1][0][0] + '(%s)'% tag_meaning[j[-1][0][1]]+ '\\n'\n",
    "            if '->' in j:\n",
    "                split_index = j.index(\"->\") \n",
    "                before = j[:split_index]\n",
    "                after = j[split_index+1:]\n",
    "                outputstring +='changed ' +before[0][0][0] +'(%s)'% tag_meaning[before[0][0][1]]\n",
    "                outputstring +='to '+after[0][0][0] +'(%s)'% tag_meaning[before[0][0][1]]+'\\n' \n",
    "    return outputstring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'do you mean: 住在哈尔滨,黑龙江哈尔滨,太冷了!\\ndeleted 了(function words)\\nadding 了(function words)\\ndo you mean: 他做梦的时候他是一个太空人。\\ndeleted 了(function words)\\ndo you mean: 走在回家的路上，开始下雨！\\ndeleted 而(conjunction)\\nadding 的(auxiliary)\\ndo you mean: 决定了是因为我住在美国的中部（所以说汉语的不多啦）。\\ndeleted 部分(common noun)\\ndo you mean: 她这个星期没教我。\\ndeleted 没(verb)\\nchanged 一样(adjective)to 没(adjective)\\ndo you mean: 考虑到这样的信息我不知道任何年轻人都可以买房子。\\nadding 到(verb)\\nadding 都(adverb)\\ndo you mean: 听说受中国的《水浒传》影响很大。\\nchanged 那(pronoun)to 受(pronoun)\\n'"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_output(inputs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3196968d684371006099b3d55edeef8ed90365227a30deaef86e5d4aa8519be0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
