{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('book_.csv')\n",
    "df = df.drop(df.columns[:2], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-26 11:31:55,417 - modelscope - INFO - Use user-specified model revision: v1.0.1\n",
      "INFO 2023-04-26 11:31:55,417 api.py:463] Use user-specified model revision: v1.0.1\n",
      "2023-04-26 11:31:55,989 - modelscope - INFO - File chinese_vocab.txt already in cache, skip downloading!\n",
      "INFO 2023-04-26 11:31:55,989 snapshot_download.py:125] File chinese_vocab.txt already in cache, skip downloading!\n",
      "2023-04-26 11:31:55,991 - modelscope - INFO - File configuration.json already in cache, skip downloading!\n",
      "INFO 2023-04-26 11:31:55,991 snapshot_download.py:125] File configuration.json already in cache, skip downloading!\n",
      "2023-04-26 11:31:55,992 - modelscope - INFO - File dict.src.txt already in cache, skip downloading!\n",
      "INFO 2023-04-26 11:31:55,992 snapshot_download.py:125] File dict.src.txt already in cache, skip downloading!\n",
      "2023-04-26 11:31:55,993 - modelscope - INFO - File dict.tgt.txt already in cache, skip downloading!\n",
      "INFO 2023-04-26 11:31:55,993 snapshot_download.py:125] File dict.tgt.txt already in cache, skip downloading!\n",
      "2023-04-26 11:31:55,995 - modelscope - INFO - File model.jpg already in cache, skip downloading!\n",
      "INFO 2023-04-26 11:31:55,995 snapshot_download.py:125] File model.jpg already in cache, skip downloading!\n",
      "2023-04-26 11:31:55,997 - modelscope - INFO - File pytorch_model.pt already in cache, skip downloading!\n",
      "INFO 2023-04-26 11:31:55,997 snapshot_download.py:125] File pytorch_model.pt already in cache, skip downloading!\n",
      "2023-04-26 11:31:55,998 - modelscope - INFO - File README.md already in cache, skip downloading!\n",
      "INFO 2023-04-26 11:31:55,998 snapshot_download.py:125] File README.md already in cache, skip downloading!\n",
      "2023-04-26 11:31:56,009 - modelscope - INFO - initiate model from C:\\Users\\Administrator\\.cache\\modelscope\\hub\\damo\\nlp_bart_text-error-correction_chinese\n",
      "INFO 2023-04-26 11:31:56,009 base.py:46] initiate model from C:\\Users\\Administrator\\.cache\\modelscope\\hub\\damo\\nlp_bart_text-error-correction_chinese\n",
      "2023-04-26 11:31:56,011 - modelscope - INFO - initiate model from location C:\\Users\\Administrator\\.cache\\modelscope\\hub\\damo\\nlp_bart_text-error-correction_chinese.\n",
      "INFO 2023-04-26 11:31:56,011 base.py:48] initiate model from location C:\\Users\\Administrator\\.cache\\modelscope\\hub\\damo\\nlp_bart_text-error-correction_chinese.\n",
      "2023-04-26 11:31:56,017 - modelscope - INFO - initialize model from C:\\Users\\Administrator\\.cache\\modelscope\\hub\\damo\\nlp_bart_text-error-correction_chinese\n",
      "INFO 2023-04-26 11:31:56,017 base_model.py:115] initialize model from C:\\Users\\Administrator\\.cache\\modelscope\\hub\\damo\\nlp_bart_text-error-correction_chinese\n",
      "INFO 2023-04-26 11:32:04,953 translation.py:317] [src] dictionary: 21132 types\n",
      "INFO 2023-04-26 11:32:04,954 translation.py:318] [tgt] dictionary: 21132 types\n",
      "2023-04-26 11:32:14,275 - modelscope - WARNING - No preprocessor field found in cfg.\n",
      "WARNING 2023-04-26 11:32:14,275 base.py:280] No preprocessor field found in cfg.\n",
      "2023-04-26 11:32:14,276 - modelscope - WARNING - No val key and type key found in preprocessor domain of configuration.json file.\n",
      "WARNING 2023-04-26 11:32:14,276 base.py:289] No val key and type key found in preprocessor domain of configuration.json file.\n",
      "2023-04-26 11:32:14,278 - modelscope - WARNING - Cannot find available config to build preprocessor at mode inference, current config: {'model_dir': 'C:\\\\Users\\\\Administrator\\\\.cache\\\\modelscope\\\\hub\\\\damo\\\\nlp_bart_text-error-correction_chinese'}. trying to build by task and model information.\n",
      "WARNING 2023-04-26 11:32:14,278 base.py:308] Cannot find available config to build preprocessor at mode inference, current config: {'model_dir': 'C:\\\\Users\\\\Administrator\\\\.cache\\\\modelscope\\\\hub\\\\damo\\\\nlp_bart_text-error-correction_chinese'}. trying to build by task and model information.\n",
      "2023-04-26 11:32:14,318 - modelscope - WARNING - No preprocessor field found in cfg.\n",
      "WARNING 2023-04-26 11:32:14,318 base.py:280] No preprocessor field found in cfg.\n",
      "2023-04-26 11:32:14,319 - modelscope - WARNING - No val key and type key found in preprocessor domain of configuration.json file.\n",
      "WARNING 2023-04-26 11:32:14,319 base.py:289] No val key and type key found in preprocessor domain of configuration.json file.\n",
      "2023-04-26 11:32:14,320 - modelscope - WARNING - Cannot find available config to build preprocessor at mode inference, current config: {'model_dir': 'C:\\\\Users\\\\Administrator\\\\.cache\\\\modelscope\\\\hub\\\\damo\\\\nlp_bart_text-error-correction_chinese'}. trying to build by task and model information.\n",
      "WARNING 2023-04-26 11:32:14,320 base.py:308] Cannot find available config to build preprocessor at mode inference, current config: {'model_dir': 'C:\\\\Users\\\\Administrator\\\\.cache\\\\modelscope\\\\hub\\\\damo\\\\nlp_bart_text-error-correction_chinese'}. trying to build by task and model information.\n"
     ]
    }
   ],
   "source": [
    "from modelscope.pipelines import pipeline\n",
    "from modelscope.utils.constant import Tasks\n",
    "from paddlenlp import Taskflow\n",
    "import macbert\n",
    "import jieba\n",
    "import difflib\n",
    "import re\n",
    "\n",
    "tagger = Taskflow(\"pos_tagging\")\n",
    "p = pipeline(Tasks.text_error_correction,\n",
    "             'damo/nlp_bart_text-error-correction_chinese', model_revision='v1.0.1')\n",
    "\n",
    "\n",
    "def format_target_list(texts):\n",
    "    target_list = [{'output': text} for text in texts]\n",
    "    return target_list\n",
    "\n",
    "# select mode\n",
    "\n",
    "\n",
    "def define_pipeline(mode='default'):\n",
    "    global p\n",
    "    if mode == 'default':\n",
    "        p = pipeline(Tasks.text_error_correction,\n",
    "                     'damo/nlp_bart_text-error-correction_chinese', model_revision='v1.0.1')\n",
    "\n",
    "    if mode == 'ernie':\n",
    "        corrector = Taskflow(\"text_correction\")\n",
    "\n",
    "        def helper(texts):\n",
    "            return format_target_list([i['target'] for i in corrector(texts)])\n",
    "\n",
    "        p = helper\n",
    "    if mode == 'macbert':\n",
    "        corrector = macbert.macbert\n",
    "\n",
    "        def helper(texts):\n",
    "            return format_target_list([i for i in corrector(texts)])\n",
    "        p = helper\n",
    "\n",
    "\n",
    "def chinese_word_segmentation(text, cut_all_mode=1):\n",
    "    seg_list = jieba.cut(text, cut_all=cut_all_mode)\n",
    "    return \" \".join(seg_list)\n",
    "\n",
    "\n",
    "def smallest_word(s_ls, t_ls):\n",
    "    for i in range(len(t_ls)):\n",
    "        for j in range(len(t_ls)):\n",
    "            if i == len(t_ls) or j == len(t_ls):\n",
    "                break\n",
    "            if i != j:\n",
    "                if t_ls[i] in t_ls[j]:\n",
    "                    t_ls.pop(j)\n",
    "\n",
    "    for i in range(len(s_ls)):\n",
    "        for j in range(len(s_ls)):\n",
    "            if i == len(s_ls) or j == len(s_ls):\n",
    "                break\n",
    "            if i != j:\n",
    "                if s_ls[i] in s_ls[j]:\n",
    "                    s_ls.pop(j)\n",
    "    return s_ls, t_ls\n",
    "\n",
    "\n",
    "def get_changes(source, target):\n",
    "    a = chinese_word_segmentation(source)\n",
    "    b = chinese_word_segmentation(target)\n",
    "\n",
    "    s_ls = [i for i in a.split() if i not in b.split()]\n",
    "    t_ls = [i for i in b.split() if i not in a.split()]\n",
    "    return smallest_word(s_ls, t_ls)\n",
    "\n",
    "\n",
    "def get_changed_word_in_sentence(sentence, char_pos):\n",
    "    seg = chinese_word_segmentation(sentence, 0)\n",
    "    count = 0\n",
    "    for i in seg.split():\n",
    "        count += len(i)\n",
    "        if count > char_pos:\n",
    "            return i\n",
    "# and type\n",
    "\n",
    "\n",
    "def get_changes_difflib(a, b, mode='char'):\n",
    "    diff = difflib.SequenceMatcher(None, a, b)\n",
    "    opcodes = diff.get_opcodes()\n",
    "    changes = []\n",
    "    if mode == 'char':\n",
    "        for tag, i1, i2, j1, j2 in opcodes:\n",
    "            if tag == 'equal':\n",
    "                continue\n",
    "            elif tag == 'delete':\n",
    "                changes.append(('-', tagger(f'{a[i1:i2]}')))\n",
    "            elif tag == 'insert':\n",
    "                changes.append(('+', tagger(f'{b[j1:j2]}')))\n",
    "            else:\n",
    "                changes.append(\n",
    "                    (tagger(f'{a[i1:i2]}'), '->', tagger(f'{b[j1:j2]}')))\n",
    "    else:\n",
    "        for tag, i1, i2, j1, j2 in opcodes:\n",
    "            if tag == 'equal':\n",
    "                continue\n",
    "            elif tag == 'delete':\n",
    "                changes.append(\n",
    "                    ('-', tagger(f'{get_changed_word_in_sentence(a,i1)}')))\n",
    "            elif tag == 'insert':\n",
    "                changes.append(\n",
    "                    ('+', tagger(f'{get_changed_word_in_sentence(b,j1)}')))\n",
    "            else:\n",
    "                changes.append((tagger(f'{get_changed_word_in_sentence(a,i1)}'),\n",
    "                               '->', tagger(f'{get_changed_word_in_sentence(b,j1)}')))\n",
    "    return changes\n",
    "\n",
    "\n",
    "def res_formatted(origin, results, mode='char'):  # input: origin text and target text\n",
    "    new_res = []\n",
    "\n",
    "    for i in range(len(origin)):\n",
    "\n",
    "        new_res.append({'source': origin[i], 'target': results[i]['output']})\n",
    "\n",
    "    for i in new_res:\n",
    "        if i['source'] == i['target']:\n",
    "            i['changes'] = ''\n",
    "        else:\n",
    "            i['changes'] = get_changes_difflib(i['source'], i['target'], mode)\n",
    "\n",
    "    return new_res\n",
    "\n",
    "\n",
    "def split_sentences(text):\n",
    "    return re.split(r'(?<=[？?！!。；;])', text)\n",
    "\n",
    "\n",
    "def get_output(inputs):\n",
    "    tag_meaning = {\n",
    "        'n': 'common noun',\n",
    "        'f': 'locative noun',\n",
    "        's': 'place noun',\n",
    "        't': 'time noun',\n",
    "        'nr': 'person name',\n",
    "        'ns': 'place name',\n",
    "        'nt': 'organization name',\n",
    "        'nw': 'work name',\n",
    "        'nz': 'other proper noun',\n",
    "        'v': 'verb',\n",
    "        'vd': 'verb adverbial',\n",
    "        'vn': 'verb noun',\n",
    "        'a': 'adjective',\n",
    "        'ad': 'adverbial adjective',\n",
    "        'an': 'adjectival noun',\n",
    "        'd': 'adverb',\n",
    "        'm': 'numeral',\n",
    "        'q': 'quantifier',\n",
    "        'r': 'pronoun',\n",
    "        'p': 'preposition',\n",
    "        'c': 'conjunction',\n",
    "        'u': 'auxiliary',\n",
    "        'xc': 'function words',\n",
    "        'w': 'punctuation',\n",
    "        'PER': 'person name',\n",
    "        'LOC': 'place name',\n",
    "        'ORG': 'organization name',\n",
    "        'TIME': 'time'\n",
    "    }\n",
    "    sentences = split_sentences(inputs)\n",
    "    for i in sentences:\n",
    "        if i == '':\n",
    "            sentences.remove(i)\n",
    "    formatted = res_formatted(sentences, p(sentences), mode='word')\n",
    "    outputstring = ''\n",
    "    for i in formatted:\n",
    "        outputstring += 'do you mean: ' + i['target']+'\\n'\n",
    "        for j in i['changes']:\n",
    "            if '-' in j:\n",
    "                outputstring += 'deleted ' + \\\n",
    "                    j[-1][0][0] + '(%s)' % tag_meaning[j[-1][0][1]]+'\\n'\n",
    "            if '+' in j:\n",
    "                outputstring += 'adding ' + \\\n",
    "                    j[-1][0][0] + '(%s)' % tag_meaning[j[-1][0][1]] + '\\n'\n",
    "            if '->' in j:\n",
    "                split_index = j.index(\"->\")\n",
    "                before = j[:split_index]\n",
    "                after = j[split_index+1:]\n",
    "                outputstring += 'changed ' + \\\n",
    "                    before[0][0][0] + '(%s)' % tag_meaning[before[0][0][1]]\n",
    "                outputstring += 'to '+after[0][0][0] + \\\n",
    "                    '(%s)' % tag_meaning[before[0][0][1]]+'\\n'\n",
    "    return outputstring\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_target_list(sentences):\n",
    "    return [i['target'] for i in res_formatted(sentences, p(sentences), mode='word')]\n",
    "\n",
    "def get_res_formatted(sentences,mode='char'):\n",
    "    return res_formatted(sentences, p(sentences),mode='char')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2023-04-26 10:43:48,313 - modelscope - INFO - PyTorch version 1.12.1+cu116 Found.\n",
      "2023-04-26 10:43:48,323 - modelscope - INFO - TensorFlow version 2.10.1 Found.\n",
      "2023-04-26 10:43:48,324 - modelscope - INFO - Loading ast index from C:\\Users\\Administrator\\.cache\\modelscope\\ast_indexer\n",
      "2023-04-26 10:43:48,661 - modelscope - INFO - Loading done! Current index file version is 1.3.0, with md5 3e9bc5fbe035d6035638439c5feaf9d4 and a total number of 746 components indexed\n",
      "c:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\_distutils_hack\\__init__.py:33: UserWarning: Setuptools is replacing distutils.\n",
      "  warnings.warn(\"Setuptools is replacing distutils.\")\n",
      "\u001b[33m[2023-04-26 10:43:53,110] [ WARNING]\u001b[0m - Detected that datasets module was imported before paddlenlp. This may cause PaddleNLP datasets to be unavalible in intranet. Please import paddlenlp before datasets module to avoid download issues\u001b[0m\n",
      "2023-04-26 10:44:02,435 - modelscope - INFO - Use user-specified model revision: v1.0.1\n",
      "INFO 2023-04-26 10:44:02,435 api.py:463] Use user-specified model revision: v1.0.1\n",
      "2023-04-26 10:44:02,817 - modelscope - INFO - File chinese_vocab.txt already in cache, skip downloading!\n",
      "INFO 2023-04-26 10:44:02,817 snapshot_download.py:125] File chinese_vocab.txt already in cache, skip downloading!\n",
      "2023-04-26 10:44:02,819 - modelscope - INFO - File configuration.json already in cache, skip downloading!\n",
      "INFO 2023-04-26 10:44:02,819 snapshot_download.py:125] File configuration.json already in cache, skip downloading!\n",
      "2023-04-26 10:44:02,820 - modelscope - INFO - File dict.src.txt already in cache, skip downloading!\n",
      "INFO 2023-04-26 10:44:02,820 snapshot_download.py:125] File dict.src.txt already in cache, skip downloading!\n",
      "2023-04-26 10:44:02,821 - modelscope - INFO - File dict.tgt.txt already in cache, skip downloading!\n",
      "INFO 2023-04-26 10:44:02,821 snapshot_download.py:125] File dict.tgt.txt already in cache, skip downloading!\n",
      "2023-04-26 10:44:02,822 - modelscope - INFO - File model.jpg already in cache, skip downloading!\n",
      "INFO 2023-04-26 10:44:02,822 snapshot_download.py:125] File model.jpg already in cache, skip downloading!\n",
      "2023-04-26 10:44:02,823 - modelscope - INFO - File pytorch_model.pt already in cache, skip downloading!\n",
      "INFO 2023-04-26 10:44:02,823 snapshot_download.py:125] File pytorch_model.pt already in cache, skip downloading!\n",
      "2023-04-26 10:44:02,824 - modelscope - INFO - File README.md already in cache, skip downloading!\n",
      "INFO 2023-04-26 10:44:02,824 snapshot_download.py:125] File README.md already in cache, skip downloading!\n",
      "2023-04-26 10:44:02,848 - modelscope - INFO - initiate model from C:\\Users\\Administrator\\.cache\\modelscope\\hub\\damo\\nlp_bart_text-error-correction_chinese\n",
      "INFO 2023-04-26 10:44:02,848 base.py:46] initiate model from C:\\Users\\Administrator\\.cache\\modelscope\\hub\\damo\\nlp_bart_text-error-correction_chinese\n",
      "2023-04-26 10:44:02,851 - modelscope - INFO - initiate model from location C:\\Users\\Administrator\\.cache\\modelscope\\hub\\damo\\nlp_bart_text-error-correction_chinese.\n",
      "INFO 2023-04-26 10:44:02,851 base.py:48] initiate model from location C:\\Users\\Administrator\\.cache\\modelscope\\hub\\damo\\nlp_bart_text-error-correction_chinese.\n",
      "2023-04-26 10:44:02,859 - modelscope - INFO - initialize model from C:\\Users\\Administrator\\.cache\\modelscope\\hub\\damo\\nlp_bart_text-error-correction_chinese\n",
      "INFO 2023-04-26 10:44:02,859 base_model.py:115] initialize model from C:\\Users\\Administrator\\.cache\\modelscope\\hub\\damo\\nlp_bart_text-error-correction_chinese\n",
      "INFO 2023-04-26 10:44:04,090 text_to_speech.py:34] Please install tensorboardX: pip install tensorboardX\n",
      "INFO 2023-04-26 10:44:15,131 translation.py:317] [src] dictionary: 21132 types\n",
      "INFO 2023-04-26 10:44:15,133 translation.py:318] [tgt] dictionary: 21132 types\n",
      "2023-04-26 10:44:23,662 - modelscope - WARNING - No preprocessor field found in cfg.\n",
      "WARNING 2023-04-26 10:44:23,662 base.py:280] No preprocessor field found in cfg.\n",
      "2023-04-26 10:44:23,663 - modelscope - WARNING - No val key and type key found in preprocessor domain of configuration.json file.\n",
      "WARNING 2023-04-26 10:44:23,663 base.py:289] No val key and type key found in preprocessor domain of configuration.json file.\n",
      "2023-04-26 10:44:23,664 - modelscope - WARNING - Cannot find available config to build preprocessor at mode inference, current config: {'model_dir': 'C:\\\\Users\\\\Administrator\\\\.cache\\\\modelscope\\\\hub\\\\damo\\\\nlp_bart_text-error-correction_chinese'}. trying to build by task and model information.\n",
      "WARNING 2023-04-26 10:44:23,664 base.py:308] Cannot find available config to build preprocessor at mode inference, current config: {'model_dir': 'C:\\\\Users\\\\Administrator\\\\.cache\\\\modelscope\\\\hub\\\\damo\\\\nlp_bart_text-error-correction_chinese'}. trying to build by task and model information.\n",
      "2023-04-26 10:44:23,709 - modelscope - WARNING - No preprocessor field found in cfg.\n",
      "WARNING 2023-04-26 10:44:23,709 base.py:280] No preprocessor field found in cfg.\n",
      "2023-04-26 10:44:23,711 - modelscope - WARNING - No val key and type key found in preprocessor domain of configuration.json file.\n",
      "WARNING 2023-04-26 10:44:23,711 base.py:289] No val key and type key found in preprocessor domain of configuration.json file.\n",
      "2023-04-26 10:44:23,712 - modelscope - WARNING - Cannot find available config to build preprocessor at mode inference, current config: {'model_dir': 'C:\\\\Users\\\\Administrator\\\\.cache\\\\modelscope\\\\hub\\\\damo\\\\nlp_bart_text-error-correction_chinese'}. trying to build by task and model information.\n",
      "WARNING 2023-04-26 10:44:23,712 base.py:308] Cannot find available config to build preprocessor at mode inference, current config: {'model_dir': 'C:\\\\Users\\\\Administrator\\\\.cache\\\\modelscope\\\\hub\\\\damo\\\\nlp_bart_text-error-correction_chinese'}. trying to build by task and model information.\n"
     ]
    }
   ],
   "source": [
    "import suggestion \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls = suggestion.get_res_formatted(df['Source'].to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['BART (Truth)(char base)']=[i['changes'] for i in ls]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls = suggestion.get_res_formatted(df['Source'].to_list(),mode='word')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['BART (Truth)(word base)']=[i['changes'] for i in ls]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['classifed (Truth)(char base)'] = [i['changes'] for i in res_formatted(df['Source'].to_list(),format_target_list(df['Truth'].to_list()))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['classifed (Truth)(word base)']= [i['changes'] for i in res_formatted(df['Source'].to_list(),format_target_list(df['Truth'].to_list()),mode='word')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('classifactions.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
